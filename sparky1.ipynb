{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-resistance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import boto3 as bt3\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import qgrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================================\n",
    "# Data Loading\n",
    "\n",
    "s3 = bt3.resource('s3')\n",
    "my_bucket = s3.Bucket('a_s3_bucket')\n",
    "s3_loc = 's3://a_s3_bucket'\n",
    "data_loc_list = ['path/to/folder/',\n",
    "  ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual list of files\n",
    "ls_loc = []\n",
    "ls_file = []\n",
    "for data_loc in data_loc_list:\n",
    "  for obj in my_bucket.objects.filter(Prefix=data_loc):\n",
    "    print(obj.key)\n",
    "    file = s3_loc + obj.key\n",
    "    ls_file.append(file)\n",
    "  print(data_loc + '\\n')\n",
    "print(\"number of files:\" + str(len(ls_file)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TC-APP1').master('local[*]') \\\n",
    ".config('spark.sql.hive.convertMetastoreParquet', 'false') \\\n",
    ".config('spark.sql.hive.caseSensitiveInferenceMode', 'NEVER_INFER') \\\n",
    ".config(fs.s3a.server-side-encryption-algorithm', 'SSE-KMS') \\\n",
    ".config(fs.s3a.server-side-encryption.key', 'alias/s3_encryption_key') \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS dbpega COMMENT 'This is DB for pega data' \").show()\n",
    "spark.sql(\"DESCRIBE DATABASE EXTENDED dbpega \").show()\n",
    "\n",
    "v_source_file_loc_path = [\"s3://a_s3_bucket/path/to/folder/\"]\n",
    "\n",
    "df_target1 = spark.read.format(\"parquet\").load(v_source_file_loc_path)\n",
    "df_target1.printSchema()\n",
    "\n",
    "# Temp tables are more resistant to a shared hive tables - avoids tampering\n",
    "df_target1.registerTempTable(\"t_check_attrib\")\n",
    "\n",
    "df_target1.columns\n",
    "display(df_target1.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
