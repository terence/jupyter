{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cathedral-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "designed-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "moral-front",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers.csv\n",
      "new_purchases.csv\n",
      "new_purchases.json\n",
      ".gitignore\n",
      "new_purchases.parquet\n"
     ]
    }
   ],
   "source": [
    "# for root, dirs, files in os.walk(\".\"):\n",
    "#    for filename in files:\n",
    "#        print(filename)\n",
    "\n",
    "for root, dir, files in os.walk(\"./data\"):\n",
    "  for filename in files:\n",
    "        print(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "frank-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "spark1 = SparkSession.builder \\\n",
    ".master('local[*]') \\\n",
    ".appName('Sparky1') \\\n",
    ".config('spark.sql.hive.convertMetastoreParquet', 'false') \\\n",
    ".config('spark.sql.hive.caseSensitiveInferenceMode', 'NEVER_INFER') \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()\n",
    "rdd = spark1.sparkContext.parallelize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "sophisticated-flight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "active-kansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "alone-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromRDD2 = spark1.createDataFrame(rdd).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "phantom-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromData2 = spark1.createDataFrame(data).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "central-grade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- apples: integer (nullable = true)\n",
      " |-- oranges: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV from Local file\n",
    "df2 = spark1.read.format('csv').options(header='true', inferSchema='true') \\\n",
    ".load('./data/new_purchases.csv')\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "interim-commons",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tables = spark1.catalog.listTables()\n",
    "display(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "treated-socket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+-------------------------+--------------------------+\n",
      "|database_description_item|database_description_value|\n",
      "+-------------------------+--------------------------+\n",
      "|            Database Name|                 purchases|\n",
      "|              Description|          DB for temp data|\n",
      "|                 Location|      file:/home/ec2-us...|\n",
      "|               Properties|                          |\n",
      "+-------------------------+--------------------------+\n",
      "\n",
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|   purchases|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark1.sql(\"CREATE DATABASE IF NOT EXISTS purchases COMMENT 'DB for temp data' \").show()\n",
    "spark1.sql(\"DESCRIBE DATABASE EXTENDED purchases \").show()\n",
    "spark1.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "prerequisite-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.mode(\"overwrite\").saveAsTable(\"purchases.test_table2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "optional-earth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|   _c0|apples|oranges|\n",
      "+------+------+-------+\n",
      "|  June|     3|      0|\n",
      "|Robert|     2|      3|\n",
      "|  Lily|     0|      7|\n",
      "| David|     1|      2|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark1.sql(\"SELECT * FROM purchases.test_table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-interface",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2 = SparkSession.builder \\\n",
    ".appName('TC-APP1') \\\n",
    ".master('local[*]') \\\n",
    ".config('spark.sql.hive.convertMetastoreParquet', 'false') \\\n",
    ".config('spark.sql.hive.caseSensitiveInferenceMode', 'NEVER_INFER') \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext\n",
    "     .hadoopConfiguration.set(\"fs.s3a.access.key\", \"awsaccesskey value\")\n",
    "service)\n",
    " // Replace Key with your AWS secret key (You can find this on IAM \n",
    "spark.sparkContext\n",
    "     .hadoopConfiguration.set(\"fs.s3a.secret.key\", \"aws secretkey value\")\n",
    "spark.sparkContext\n",
    "      .hadoopConfiguration.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "\n",
    "# SparkConf().getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-burner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
